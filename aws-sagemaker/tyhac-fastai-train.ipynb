{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# AWS libs and setup\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.Session().client('sagemaker')\n",
    "\n",
    "print(\"Region = {}\".format(region))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!sudo apt-get install -y libsndfile1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile training.py\n",
    "import argparse\n",
    "import glob\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from sagemaker_training import environment\n",
    "from fastai.vision.all import *\n",
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *\n",
    "from fastaudio.ci import skip_if_ci\n",
    "import torchaudio\n",
    "import pathlib\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "# import ipywidgets as widgets\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# source: https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/fastai_oxford_pets/source/pets.py\n",
    "\n",
    "def _train(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(\"Device Type: {}\".format(device))\n",
    "    print(\"Device Type: {}\".format(device))\n",
    "\n",
    "    logger.info(\"Loading tyhac dataset\")\n",
    "    print(f\"Batch size: {args.batch_size}\")\n",
    "    path = Path(args.data_dir)\n",
    "    print(f\"Data path is: {path}\")\n",
    "    \n",
    "    data_csv = path / 'tyhac_combined_cleaned.csv'\n",
    "    df = pd.read_csv(data_csv)\n",
    "    \n",
    "    # remap the file_path for the training container\n",
    "    df.file_path.replace('rawdata/combined/', '/opt/ml/input/data/training/', inplace=True, regex=True)\n",
    "\n",
    "    # Isolate testing data for later\n",
    "    dataset = df.sample(frac=0.9,random_state=200)\n",
    "    dataset_test = df.drop(dataset.index) # ~1k\n",
    "    dataset_test.to_csv(Path(args.model_dir) / 'dataset_test.csv') # save for more testing\n",
    "    print(f\"Dataset train shape is: {dataset.shape}\")\n",
    "    print(f\"Dataset test shape is: {dataset_test.shape}\")\n",
    "    \n",
    "    # configuration for audio processing\n",
    "    # TODO: move these to args\n",
    "    n_fft=2048 # 20*n_mels (128 default)\n",
    "    hop_length=512\n",
    "    target_rate=48000\n",
    "    n_mels=128\n",
    "    \n",
    "    # Setup fastaudio\n",
    "    cfg = AudioConfig.BasicMelSpectrogram(n_fft=n_fft, hop_length=hop_length, sample_rate=target_rate)\n",
    "    a2s = AudioToSpec.from_cfg(cfg)\n",
    "    print(f\"Fastaudio hyperparameters: {cfg}\")\n",
    "\n",
    "    # crop1s = ResizeSignal(6000) # milliseconds\n",
    "    # pipe = Pipeline([AudioTensor.create, crop1s, a2s])\n",
    "\n",
    "    # Split out data into train and validation. Remember we've already isolated some for test.\n",
    "    labels = dataset.covid.values\n",
    "    test_size=0.2\n",
    "    splitter = TrainTestSplitter(test_size=test_size, random_state=42, stratify=labels)\n",
    "    print(f\"Dataset label counts: {Counter(labels)}\")\n",
    "    \n",
    "    db = DataBlock(blocks=(AudioBlock, CategoryBlock),\n",
    "        get_x=ColReader('file_path'),\n",
    "        get_y=ColReader('covid'),\n",
    "        splitter=splitter,\n",
    "        item_tfms=[ResizeSignal(6000), a2s]) # milliseconds\n",
    "    \n",
    "    dsets = db.datasets(dataset)\n",
    "    \n",
    "    # Our dataset is imbalanced, attempt to add weights\n",
    "    count = Counter(labels)\n",
    "    # AudioTensor, CategoryTensor\n",
    "    wgts = [1/count[dsets.vocab[label]] for img, label in dsets.train]\n",
    "    wgts[:10]\n",
    "    \n",
    "    # Load data with weights\n",
    "    dls = db.dataloaders(dataset, num_workers=args.workers, dl_type=WeightedDL, wgts=wgts, bs=args.batch_size)\n",
    "    \n",
    "    # Model\n",
    "    print(f\"Model architecture is {args.model_arch}\")\n",
    "    arch = getattr(models, args.model_arch)\n",
    "    metrics = [accuracy, error_rate, RocAuc()]\n",
    "    # n_in = number of channels for img\n",
    "    learn = cnn_learner(dls, arch, n_in=1, metrics=[accuracy, error_rate])\n",
    "    # model learning\n",
    "    # learn.fine_tune(args.epochs, freeze_epochs=2) # my orig\n",
    "    learn.fine_tune(args.epochs) # my orig\n",
    "    #print(\"Fit for 4 cycles\")\n",
    "    #learn.fit_one_cycle(args.epochs)\n",
    "    #learn.unfreeze()\n",
    "    #print(\"Unfreeze and fit for another 2 cycles\")\n",
    "    #learn.fit_one_cycle(2, lr_max=slice(1e-6, 1e-4))\n",
    "    print(\"Finished Training\")\n",
    "    logger.info(\"Saving the model.\")\n",
    "    model_path = Path(args.model_dir)\n",
    "    # print(f\"Export data object\")\n",
    "    learn.export(model_path / \"model.pth\")\n",
    "\n",
    "    # Model performance\n",
    "    # https://www.kaggle.com/qitvision/a-complete-ml-pipeline-fast-ai\n",
    "    preds,y, loss = learn.get_preds(with_loss=True)\n",
    "    # get accuracy\n",
    "    acc = accuracy(preds, y)\n",
    "    print('The accuracy is {0} %.'.format(acc))\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    # probs from log preds\n",
    "    probs = np.exp(preds[:,1])\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y, probs, pos_label=1)\n",
    "\n",
    "    # Compute ROC area\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('ROC area is {0}'.format(roc_auc))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(model_path / 'roc_curve.jpg', dpi=70, bbox_inches='tight', pad_inches=1)\n",
    "    plt.close()   \n",
    "\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n",
    "    plt.savefig(model_path / 'confusion_matrix.jpg', dpi=70, bbox_inches='tight', pad_inches=1)\n",
    "    plt.close()   \n",
    "    \n",
    "    # Report\n",
    "    classdf = pd.DataFrame(interp.print_classification_report()).transpose()\n",
    "    classdf\n",
    "    classdf.to_csv(model_path / 'classification_report.csv')\n",
    "    \n",
    "    # Losses\n",
    "    # losses = learn.recorder.plot_losses(return_fig=True)\n",
    "    # losses.savefig(model_path / 'loss.png')\n",
    "\n",
    "    # Did not work\n",
    "    # lr_find() - gives you suggested\n",
    "    # learn.lr_find()\n",
    "    #graph = learn.recorder.plot(return_fig=True)\n",
    "    #graph.savefig(model_path / 'learning.png')\n",
    "    \n",
    "    # create empty models dir\n",
    "    os.mkdir(model_path / \"models\")\n",
    "    print(f\"Saving model weights\")\n",
    "    return learn.save(model_path / f\"{args.model_arch}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--workers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        metavar=\"W\",\n",
    "        help=\"number of data loading workers (default: 2)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        metavar=\"E\",\n",
    "        help=\"number of total epochs to run (default: 2)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=64, metavar=\"BS\", help=\"batch size (default: 4)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning-rate\",\n",
    "        type=float,\n",
    "        default=0.001,\n",
    "        metavar=\"LR\",\n",
    "        help=\"initial learning rate (default: 0.001)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--momentum\", type=float, default=0.9, metavar=\"M\", help=\"momentum (default: 0.9)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-arch\",\n",
    "        type=str,\n",
    "        default=\"resnet34\",\n",
    "        metavar=\"MA\",\n",
    "        help=\"model arch (default: resnet34)\",\n",
    "    )\n",
    "\n",
    "    env = environment.Environment()\n",
    "    parser.add_argument(\"--hosts\", type=list, default=env.hosts)\n",
    "    parser.add_argument(\"--current-host\", type=str, default=env.current_host)\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=env.model_dir)\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=env.channel_input_dirs.get(\"training\"))\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=env.num_gpus)\n",
    "\n",
    "    _train(parser.parse_args())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rawbucket = sess.default_bucket() # Alternatively you can use our custom bucket here. \n",
    "prefix = 'sagemaker-covid-tyhac-fastai' # use this prefix to store all files pertaining to this workshop.\n",
    "traindataprefix = prefix + '/data'\n",
    "train_input_path = \"s3://{}/{}/\".format(rawbucket, traindataprefix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hyperparams = { 'epochs'       : 1,\n",
    "                'learning-rate': 0.001, # fastai default\n",
    "                'batch-size'   : 64, # fastaudio default\n",
    "                'model-arch'   : 'resnet18', # resnet34\n",
    "                'workers'      : 16 # default 2\n",
    "              }\n",
    "\n",
    "# Versions: https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "tyhac_estimator = PyTorch(entry_point         = 'training.py',\n",
    "                         base_job_name        = 'tyhac-fastai',\n",
    "                         role                 = role,\n",
    "                         max_wait             = 21600, # need for spot, >= max_run, 6hrs\n",
    "                         max_run              = 21600, # seconds\n",
    "                         output_path          = 's3://{}/{}/models'.format(rawbucket, prefix),\n",
    "                         sagemaker_session    = sess,\n",
    "                         use_spot_instances   = True,\n",
    "                         script_mode          = True,\n",
    "                         framework_version   = '1.0.0', # tyhac version, needed for deploy\n",
    "                         #py_version          = 'py36',\n",
    "                         image_uri            = 'ACCOUNTID.dkr.ecr.ap-southeast-2.amazonaws.com/tyhac-sagemaker-fastai:1.0-gpu-py36',\n",
    "                         instance_count       = 1,\n",
    "                         instance_type        = 'ml.g4dn.4xlarge', # ml.g4dn.xlarge # ml.g4dn.4xlarge $1.24 # ~ $0.7364 p/h (per/s billing, 60s min)\n",
    "                         hyperparameters      = hyperparams) \n",
    "\n",
    "tyhac_estimator.fit(train_input_path, wait=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_out = tyhac_estimator.output_path + '/' + tyhac_estimator.latest_training_job.job_name + \"/output/model.tar.gz\"\n",
    "!mkdir {tyhac_estimator.latest_training_job.job_name}\n",
    "!aws s3 cp {model_out} {tyhac_estimator.latest_training_job.job_name}\n",
    "filename = tyhac_estimator.latest_training_job.job_name + '/' + 'model.tar.gz'\n",
    "!tar -zxvf {filename} -C {tyhac_estimator.latest_training_job.job_name} 'export.pkl' 'roc_curve.jpg' 'confusion_matrix.jpg' 'dataset_test.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Deploying ...\")\n",
    "\n",
    "predictor = tyhac_estimator.deploy(\n",
    "    endpoint_name=\"tyhac-endpoint-fastai\",\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium', # $0.056 p/h\n",
    "    wait=False\n",
    ")\n",
    "\n",
    "print(\"Predictor endpoint name : %s\" % predictor.endpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Delete to save costs\n",
    "predictor.delete_endpoint()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tyhac_estimator.model_data)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}