{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preparation\n",
    "The purpose of this notebook is to prepare out data. The data will be downloaded from various sources and then we'll use sagemaker to run preprocessing and prepare the data for training.\n",
    "\n",
    "This notebook should ideally only be run a couple of times, the result should output to your s3 bucket so then for training you can just pull the data during training each time. Rather then preprocess and train each time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!sudo apt-get install -y sox unzip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# AWS libs and setup\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "sm = boto3.Session().client('sagemaker')\n",
    "\n",
    "print(\"Region = {}\".format(region))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# \n",
    "# Download the data\n",
    "# Note: You'll only want to do this once as it takes a while to download the source data\n",
    "#\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# \n",
    "# Initialze folder structure\n",
    "#\n",
    "\n",
    "coswara_data = 'rawdata/coswara'\n",
    "coughvid_data = 'rawdata/coughvid'\n",
    "combined_data = 'rawdata/combined'\n",
    "combined_audio_data = 'rawdata/combined/audio'\n",
    "\n",
    "if not os.path.exists('rawdata'):\n",
    "    !mkdir rawdata\n",
    "    !mkdir {coughvid_data}\n",
    "    !mkdir {combined_data}\n",
    "    !mkdir {combined_audio_data}\n",
    "\n",
    "#\n",
    "# Coswara\n",
    "#\n",
    "# Note: These are already in wav format in an archive, the labeling is more complex it's not just coughs, also speaking\n",
    "# Extract and processing: https://github.com/iiscleap/Coswara-Data/blob/master/extract_data.py\n",
    "# Size: ~12.7GB\n",
    "\n",
    "# Let's check in case we've already done this\n",
    "extracted_data_dir = os.path.join(coswara_data, 'extracted_data')\n",
    "if not os.path.exists(extracted_data_dir):\n",
    "    print(\"no coswara dir\")\n",
    "    !git clone https://github.com/iiscleap/Coswara-Data.git {coswara_data}  \n",
    "\n",
    "    if not os.path.exists(extracted_data_dir):\n",
    "        os.makedirs(extracted_data_dir) # Creates the Extracted_data folder if it doesn't exist\n",
    "\n",
    "    dirs_extracted = set(map(os.path.basename,glob.glob('{}/202*'.format(extracted_data_dir))))\n",
    "    dirs_all = set(map(os.path.basename,glob.glob('{}/202*'.format(coswara_data))))\n",
    "\n",
    "    dirs_to_extract = list(set(dirs_all) - dirs_extracted)\n",
    "\n",
    "    for d in dirs_to_extract:\n",
    "        p = subprocess.Popen('cat {}/{}/*.tar.gz.* |tar -xz -C {}/'.format(coswara_data, d, extracted_data_dir), shell=True)\n",
    "        p.wait()\n",
    "else:\n",
    "    print(\"found exisiting coswara dataset\")\n",
    "\n",
    "# Format the extracted contents and csv\n",
    "# I had issues with pandas and the word \"true\", just couldn't select it?\n",
    "if os.path.isfile(os.path.join(coswara_data, 'combined_data_upd.csv')):\n",
    "    os.remove(os.path.join(coswara_data, 'combined_data_upd.csv'))\n",
    "text = open(os.path.join(coswara_data, 'combined_data.csv'), \"r\")\n",
    "text = ''.join([i for i in text]) \\\n",
    "    .replace(\"True\", \"IsTrue\")\n",
    "x = open(os.path.join(coswara_data, 'combined_data_upd.csv'), \"w\")\n",
    "x.writelines(text)\n",
    "x.close()\n",
    "\n",
    "# Filter out records:\n",
    "#   Covid Status = positive or negative\n",
    "#   Cough = True\n",
    "coswara_pd = pd.read_csv(os.path.join(coswara_data, 'combined_data_upd.csv'), header=0)\n",
    "coswara_pd = coswara_pd[['id', 'covid_status']]\n",
    "# drop nans\n",
    "coswara_pd.dropna(inplace=True)\n",
    "coswara_pd['file_path'] = \"\"\n",
    "coswara_pd['source'] = \"\"\n",
    "\n",
    "# Add the file_path to the csv file\n",
    "for filename in glob.glob(extracted_data_dir + '/*/*/cough-shallow.wav'):\n",
    "    file_uid = filename.split('/')[-2]\n",
    "    file_path = filename\n",
    "    coswara_pd.loc[coswara_pd.id == file_uid, \"file_path\"] = filename\n",
    "    # adding a source col\n",
    "    coswara_pd.loc[coswara_pd.id == file_uid, \"source\"] = \"coswara\"\n",
    "    \n",
    "coswara_pd.loc[coswara_pd.covid_status == \"COVID-19\", \"covid_status\"] = \"positive\"\n",
    "coswara_pd.loc[coswara_pd.covid_status == \"positive\", \"covid_status\"] = \"positive\"\n",
    "coswara_pd.loc[coswara_pd.covid_status == \"positive_mild\", \"covid_status\"] = \"positive\"\n",
    "coswara_pd.loc[coswara_pd.covid_status == \"positive_moderate\", \"covid_status\"] = \"positive\"\n",
    "coswara_pd.loc[coswara_pd.covid_status == \"positive_asymp\", \"covid_status\"] = \"positive\"\n",
    "# At this point, we've identified the positive cases, everything else is a form of negative, for this\n",
    "# set i am using healthy only. There are some others like recovered etc but small subset only.\n",
    "coswara_pd.loc[coswara_pd.covid_status == \"healthy\", \"covid_status\"] = \"negative\"\n",
    "coswara_pd = coswara_pd[coswara_pd.covid_status.isin(['negative', 'positive'])]\n",
    "# standardize columns\n",
    "coswara_pd.rename(columns={\"id\":\"uuid\",\"covid_status\":\"covid\"},inplace=True)\n",
    "\n",
    "# include heavy coughs\n",
    "# copy the df over, replace the word for the filepath, join the two df's together\n",
    "heavy = coswara_pd.copy()\n",
    "heavy[\"file_path\"].replace(\"shallow\", \"heavy\", inplace=True, regex=True)\n",
    "coswara_pd = pd.concat([coswara_pd, heavy])\n",
    "\n",
    "# save to csv\n",
    "if os.path.isfile(os.path.join(coswara_data,\"tyhac_coswara_stripped.csv\")):\n",
    "    os.remove(os.path.join(coswara_data,\"tyhac_coswara_stripped.csv\"))\n",
    "coswara_pd.to_csv(os.path.join(coswara_data,\"tyhac_coswara_stripped.csv\"), index=None)\n",
    "# coswara_pd\n",
    "\n",
    "print(\"coswara extraction process completed!\")\n",
    "\n",
    "#\n",
    "# Coughvid - zenodo\n",
    "#\n",
    "# Note: Crowdsourced dataset with covid status, contains ogg, webm, csv, json\n",
    "# Size: ~1.3GB\n",
    "\n",
    "# Let's check in case we've already done this\n",
    "if not os.path.exists(coughvid_data + '/public_dataset'):\n",
    "    print(\"no coughvid dir\")\n",
    "    !wget -O \"{coughvid_data}/public_dataset.zip\" \"https://zenodo.org/record/4498364/files/public_dataset.zip?download=1\"\n",
    "    # extract the dataset\n",
    "    !unzip -q \"{coughvid_data}/public_dataset.zip\" -d {coughvid_data}\n",
    "else:\n",
    "    print(\"found exisiting coughvid dataset\")\n",
    "    \n",
    "extracted_data_dir = os.path.join(coughvid_data, 'public_dataset')\n",
    "\n",
    "#\n",
    "# Coughvid - processing\n",
    "# Reformat for the bits we want\n",
    "#\n",
    "# Filter out records:\n",
    "#  Covid Status = positive or negative\n",
    "#  Cough = True\n",
    "coughvid_pd = pd.read_csv(os.path.join(extracted_data_dir,'metadata_compiled.csv'),header=0)\n",
    "# filer the set for coughs and columns we want\n",
    "coughvid_pd = coughvid_pd.loc[coughvid_pd['cough_detected'] >= 0.9][['uuid', 'status']]\n",
    "# strip the nans\n",
    "coughvid_pd.dropna(inplace=True)\n",
    "\n",
    "# Healthy or COVID-19\n",
    "coughvid_pd.loc[coughvid_pd.status == \"COVID-19\", \"status\"] = \"positive\"\n",
    "coughvid_pd.loc[coughvid_pd.status == \"positive\", \"status\"] = \"positive\"\n",
    "coughvid_pd.loc[coughvid_pd.status == \"positive_mild\", \"status\"] = \"positive\"\n",
    "coughvid_pd.loc[coughvid_pd.status == \"positive_moderate\", \"status\"] = \"positive\"\n",
    "coughvid_pd.loc[coughvid_pd.status == \"positive_asymp\", \"status\"] = \"positive\"\n",
    "# At this point, we've identified the positive cases, everything else is a form of negative\n",
    "coughvid_pd.loc[coughvid_pd.status == \"healthy\", \"status\"] = \"negative\"\n",
    "coughvid_pd = coughvid_pd[coughvid_pd.status.isin(['negative', 'positive'])]\n",
    "\n",
    "coughvid_pd['file_path'] = \"\"\n",
    "coughvid_pd['source'] = \"\"\n",
    "\n",
    "# Add the file_path to the csv file\n",
    "for filename in glob.glob(extracted_data_dir + '/*'):\n",
    "    fullfile = filename.split('/')[-1]\n",
    "    file_uid = fullfile.split('.')[0]\n",
    "    file_ext = fullfile.split('.')[1]\n",
    "    if file_ext == 'ogg' or file_ext == 'webm':\n",
    "        file_path = filename\n",
    "        coughvid_pd.loc[coughvid_pd.uuid == file_uid, \"file_path\"] = filename\n",
    "        coughvid_pd.loc[coughvid_pd.uuid == file_uid, \"source\"] = 'coughvid'\n",
    "\n",
    "# standardize columns\n",
    "coughvid_pd.rename(columns={\"status\":\"covid\"},inplace=True)\n",
    "\n",
    "# save to csv\n",
    "if os.path.isfile(os.path.join(coughvid_data,\"tyhac_coughvid_stripped.csv\")):\n",
    "    os.remove(os.path.join(coughvid_data,\"tyhac_coughvid_stripped.csv\"))\n",
    "coughvid_pd.to_csv(os.path.join(coughvid_data,\"tyhac_coughvid_stripped.csv\"), index=None)\n",
    "\n",
    "print(\"coughvid extraction process completed!\")\n",
    "\n",
    "#\n",
    "# Merge the datasets to a single csv\n",
    "#\n",
    "combined = pd.concat([coswara_pd, coughvid_pd])\n",
    "# ditch NaN, somehow it had some?\n",
    "combined.dropna(inplace=True)\n",
    "# Note: temporary to limit post processing during testing\n",
    "# combined = combined.head(150)\n",
    "if os.path.isfile(os.path.join(combined_data,\"tyhac_combined.csv\")):\n",
    "    os.remove(os.path.join(combined_data,\"tyhac_combined.csv\"))\n",
    "combined.to_csv(os.path.join(combined_data,\"tyhac_combined.csv\"), index=None)\n",
    "\n",
    "print(\"download, extraction, csv completed\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# \n",
    "# Add in cough heavy and shallow, do some file conversion to wav format\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "import torchaudio\n",
    "\n",
    "combined_audio_data = 'rawdata/combined/audio'\n",
    "combined_data = 'rawdata/combined'\n",
    "combined_csv = os.path.join(combined_data,\"tyhac_combined.csv\")\n",
    "combined_pd = pd.read_csv(combined_csv)\n",
    "combined_pd.dropna(inplace=True) # I had already done this above but there was a NaN that surfaced, a missing file?\n",
    "\n",
    "# Some of the files are named cough-*.wav, we want to name them the uuid instead\n",
    "def find_filename(file_path):\n",
    "    ext = file_path.split('.')[-1]\n",
    "    # I've kept cough-heavy because one of the sets contains soft and heavy coughs for a single uuid\n",
    "    # should be able to expand on this if needed\n",
    "    if 'cough-heavy' in file_path:\n",
    "        uuid = file_path.split('/')[-2]\n",
    "        new_file = uuid + '-cough-heavy.' + ext\n",
    "    elif 'cough-shallow' in file_path:\n",
    "        uuid = file_path.split('/')[-2]\n",
    "        new_file = uuid + '-cough-shallow.' + ext\n",
    "    else:\n",
    "        # just send back the filename\n",
    "        new_file = file_path.split('/')[-1]\n",
    "    return new_file\n",
    "\n",
    "# Copy file from source data set into combined folder structure\n",
    "def copy_file():\n",
    "    files = combined_pd['file_path']\n",
    "    for file in files:\n",
    "        new_file = find_filename(file)\n",
    "        copyfile(file, os.path.join(combined_audio_data, new_file))\n",
    "\n",
    "def convert_files(source, dest):\n",
    "    \"\"\"Convert files from .webm and .ogg to .wav\n",
    "    folder: path to coughvid database and metadata_compiled csv\"\"\"\n",
    "    \n",
    "    names_to_convert = combined_pd.uuid.to_numpy()\n",
    "    for counter, name in enumerate(names_to_convert):\n",
    "        if (counter%1000 == 0):\n",
    "            print(\"Finished {0}/{1}\".format(counter,len(names_to_convert)))\n",
    "        if os.path.isfile(source + name + '.webm'):\n",
    "            subprocess.call([\"ffmpeg\", \"-nostats\", \"-loglevel\", \"error\", \"-n\", \"-i\", source+name+\".webm\", dest+name+\".wav\"])\n",
    "            os.remove(source + name + '.webm')\n",
    "        elif os.path.isfile(source + name + '.ogg'):\n",
    "            subprocess.call([\"ffmpeg\", \"-nostats\", \"-loglevel\", \"error\", \"-n\", \"-i\", source+name+\".ogg\", dest+name+\".wav\"])\n",
    "            os.remove(source + name + '.ogg')\n",
    "\n",
    "def resample(source, dest):\n",
    "    # resample, majority of the dataset is 48000, resample everything else to 48000\n",
    "    for fn in glob.glob(source + '*.*'):\n",
    "        # set some vars for filenames and extensions\n",
    "        filenoext = fn.split('.')\n",
    "        fileext = filenoext[1]\n",
    "        filenoext = filenoext[0]\n",
    "        filetemp = filenoext + '_temp.wav'\n",
    "                           \n",
    "        # whatever is left needs resampling\n",
    "        y, sr = torchaudio.load(fn) # access sample sr etc\n",
    "        # print(filenoext)\n",
    "        if sr != 48000:\n",
    "            print(\"resample: \", fn)\n",
    "            # print(\"would have temped: \", filetemp)\n",
    "            subprocess.call([\"sox\", fn, \"-r\", \"48000\", filetemp])\n",
    "            # remove the old non 48k sample\n",
    "            os.remove(fn)\n",
    "            # rename temp back to orig filename\n",
    "            os.rename(filetemp, fn)\n",
    "\n",
    "# The files have a new location, need to update the sheet\n",
    "def update_csv(combined_csv):\n",
    "    # Reset\n",
    "    combined_pd = pd.read_csv(combined_csv)\n",
    "    combined_pd.dropna(inplace=True) # I had already done this above but there was a NaN that surfaced, a missing file?\n",
    "    for filename in glob.glob(combined_audio_data + '/*.wav'):\n",
    "        file_uid = filename.split('/')[-1]\n",
    "        # handle our special naming and grab uuid\n",
    "        if 'cough' in file_uid:\n",
    "            file_split = file_uid.split('-') # uid/cough-shallow.wav, find by path\n",
    "            file_uid = file_split[0]\n",
    "            file_cough = file_split[2]\n",
    "            old_file = file_uid + '/cough-' + file_cough\n",
    "            rowIndex = combined_pd[combined_pd.file_path.str.contains(old_file)]\n",
    "            combined_pd.loc[combined_pd.index.values == rowIndex.index.values, \"file_path\"] = filename\n",
    "            # print(rowIndex.index.values[0])\n",
    "            #\n",
    "        else:\n",
    "            # this will be .wav\n",
    "            file_uid = file_uid.split('.')[0]\n",
    "            # Simple unique lookup\n",
    "            combined_pd.loc[combined_pd.uuid == file_uid, \"file_path\"] = filename\n",
    "        combined_pd.to_csv(combined_csv, index=None)\n",
    "    print(\"csv combine updated\")\n",
    "    \n",
    "# Run fun\n",
    "copy_file()\n",
    "convert_files(combined_audio_data + '/', combined_audio_data + '/')\n",
    "resample(combined_audio_data + '/', combined_audio_data + '/')\n",
    "update_csv(combined_csv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# \n",
    "# Remove zero length files. I found that during initial training there incorrect shape files. I verified this by\n",
    "# also trying to playback the audio which failed. this simply identifies these records and cuts them from the list.\n",
    "#\n",
    "import pandas as pd\n",
    "dataset_csv = pd.read_csv('rawdata/combined/tyhac_combined.csv')\n",
    "\n",
    "files = dataset_csv\n",
    "\n",
    "for index, row in files.iterrows():\n",
    "    record = files.loc[files['file_path'] == row['file_path']]\n",
    "    try:\n",
    "        y, sr = torchaudio.load(record.file_path.values[0])\n",
    "        # Remove the 0 length files\n",
    "        if y.shape[1] == 0:\n",
    "            print(\"wav error: \", record.index.values[0], \" uuid: \", row['uuid'])\n",
    "            files = files[files.file_path != row['file_path']]\n",
    "    except:\n",
    "        # Identified during shallow / heavy split, non issue now\n",
    "        print(\"null record error: \", row['uuid'])\n",
    "\n",
    "# Write out the clean file so we don't have to keep doing this\n",
    "if os.path.isfile(os.path.join(combined_data,\"tyhac_combined_cleaned.csv\")):\n",
    "    os.remove(os.path.join(combined_data,\"tyhac_combined_cleaned.csv\"))\n",
    "    \n",
    "files.to_csv('rawdata/combined/tyhac_combined_cleaned.csv', index=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# \n",
    "# Upload to s3\n",
    "#\n",
    "rawbucket = sess.default_bucket() # Alternatively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-covid-tyhac-fastai' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/data'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Upload all the files\n",
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data('rawdata/combined', bucket=rawbucket, key_prefix=dataprefix)\n",
    "print(raw_data_location)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This part is just data viz\n",
    "from fastai.vision.all import *\n",
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *\n",
    "from fastaudio.ci import skip_if_ci\n",
    "import torchaudio\n",
    "import pathlib\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# configuration for audio processing\n",
    "n_fft=2048 # 20*n_mels (128 default)\n",
    "hop_length=512\n",
    "target_rate=48000\n",
    "n_mels=128\n",
    "\n",
    "# Setup fastaudio\n",
    "cfg = AudioConfig.BasicMelSpectrogram(n_fft=n_fft, hop_length=hop_length, sample_rate=target_rate, n_mels=n_mels)\n",
    "a2s = AudioToSpec.from_cfg(cfg)\n",
    "print(f\"Fastaudio hyperparameters: {cfg}\")\n",
    "\n",
    "# dataset\n",
    "dataset = pd.read_csv('rawdata/combined/tyhac_combined_cleaned.csv')\n",
    "\n",
    "# match the pipeline for training\n",
    "crop1s = ResizeSignal(6000) # milliseconds\n",
    "pipe = Pipeline([AudioTensor.create, crop1s, a2s])\n",
    "\n",
    "def stft_and_display(signal, n_fft=512, hop_length=128, to_db_scale=False, n_mels=128, mel_scale=False, \n",
    "                     top_db=80, show_shape=False):\n",
    "    stft = librosa.stft(signal, n_fft, hop_length)\n",
    "    real_portion = abs(stft)\n",
    "    if(mel_scale):   real_portion = librosa.feature.melspectrogram(S=real_portion, n_fft=n_fft, n_mels=n_mels)\n",
    "    if(to_db_scale): real_portion = librosa.amplitude_to_db(real_portion, top_db)\n",
    "    if(show_shape):  print(\"Shape: {}x{}\".format(*real_portion.shape))\n",
    "    plt.imshow(real_portion)\n",
    "    plt.show()\n",
    "    \n",
    "# This will show the actuals, e.g. what the algo will be learning from\n",
    "for fn in dataset.file_path.values[:3]:\n",
    "    y, sr = librosa.load(fn, sr=target_rate)\n",
    "    stft_and_display(y, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, mel_scale=True, to_db_scale=True, show_shape=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show some samples\n",
    "for fn in dataset.file_path.values[:3]:\n",
    "    # y, sr = torchaudio.load(fn) # access sample sr etc\n",
    "    audio = AudioTensor.create(fn)\n",
    "    audio.show()\n",
    "    pipe(fn).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = dataset.covid.values\n",
    "test_size=0.2\n",
    "splitter = TrainTestSplitter(test_size=test_size, random_state=42, stratify=labels)\n",
    "Counter(labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "db = DataBlock(\n",
    "    blocks=(AudioBlock, CategoryBlock),\n",
    "    get_x=ColReader('file_path'),\n",
    "    get_y=ColReader('covid'),\n",
    "    splitter=splitter,\n",
    "    item_tfms=[ResizeSignal(6000), a2s])\n",
    "\n",
    "dsets = db.datasets(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Imbalance\n",
    "count = Counter(labels)\n",
    "\n",
    "# AudioTensor, CategoryTensor\n",
    "wgts = [1/count[dsets.vocab[label]] for img, label in dsets.train]\n",
    "wgts[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Default bs = 64, won't work on small set, add bs=x.n \n",
    "# num_workers greater than 1 had issues on local book\n",
    "dls = db.dataloaders(dataset, num_workers=1, dl_type=WeightedDL, wgts=wgts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking the balance\n",
    "x, y = dls.one_batch()\n",
    "sum(y)/len(y)\n",
    "## ~50% => we are fine here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dls.show_batch(ncols=3, nrows=2, figsize=(20, 10))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (SageMaker JumpStart TensorFlow 1.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:163294280517:image/sagemaker-jumpstart-tensorflow-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}